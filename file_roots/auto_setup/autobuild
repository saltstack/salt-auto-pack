#!/bin/bash

# ensure /srv/pillar/auto_setup/ exists
# write pillar sls file with branch_tag and build_dsig in it
# write new top.sls file containing it,
# then everyone can get it from that sls file and pillar get

## helper functions

_timestamp() {
    date "+%Y-%m-%d %H:%M:%S:"
}

_log() {
    echo "$1" | sed "s/^/$(_timestamp) /" >>"${LOGGING}"
}

# Both echo and log
_display() {
    echo "$1"
    _log "$1"
}

_error() {
    msg="ERROR: $1"
    echo "$msg" 1>&2
    echo "$(_timestamp) $msg" >>"${LOGGING}"
    echo "One or more errors found. See ${LOGGING} for details." 1>&2
    exit 1
}

_warning() {
    msg="WARNING: $1"
    echo "$msg" 1>&2
    echo "$(_timestamp) $msg" >>"${LOGGING}"
}

## updated to Neon and Post Neon where versioning is now 3000 plus based and no longer YYYY.MM based
## if earlier YYYY.MM builds are required, utilize salt_auto_pack master branch with pre-neon tag

_usage() {
    echo ""
    echo "usage: ${0}  [-h|--help] [-b|--branch <branch to build>] [-c|--clean]"
    echo "             [-g|--cloud_map <specific cloud map>] [-j|--cloud_hold]"
    echo "             [-m|--minion <minion to use>] [-n|--named_branch <code named branch to build>]"
    echo "             [-r|--nfs_opts <NFS server's directories>] [-o|--user_salt_only]"
    echo "             [-p|--pack_branch <git named branch>] [-q|--python2]"
    echo "             [-s|--specific_name <specific named version to produce>]"
    echo "             [-u|--user <username for git salt and salt-pack>] [-v|--verbose]"
    echo "             [-w|--mount_nfsdir <mount root NFS directory for minions>"
    echo "             [-y|--nfs_host <hostname of NFS server>]"
    echo "             [-z|--nfs_absdir <absolute NFS directory on NFS server for build product>]"
    echo ""
    echo "  -b, --branch        git HEAD of branch for intended or specified major version, default master"
    echo "                      if specific_name user is not used then, salt-pack branch version is used \(for example: nightly build\)"
    echo "  -c, --clean         clean build, do not not use any dependencies already built for the branch, default not clean"
##    echo "  -d, --debug     debug output enabled"
    echo "  -g, --cloud_map     cloud map to overwrite default build minions to use, default '/etc/salt/cloud.map'"
    echo "  -h, --help          this message"
    echo "  -j, --cloud_hold    default is to remove cloud files, cloud.map, cloud.profiles, cloud.providers, switch preserves them"
##    echo "  -l, --log       logging mode"
    echo "  -m, --minion        salt-minion installed on salt-master node to use for code checkout, default 'm7m'"
    echo "  -n, --named_branch  git named branch for example: nitrogen, my_user_branch1, no default"
    echo "  -o, --user_salt_only only use username git's salt for build, used in conjunction with --user, default false"
    echo "  -p, --pack_branch   name of salt-pack branch to use, default master"
    echo "  -q, --python2       Python version to build, default Python 3 (false), Python2 set to true"
    echo "  -r, --nfs_opts      NFS options used to mount NFS server's directories"
    echo "  -s, --specific_name specifically named version to build, default dated autobuild, for example: rc1"
    echo "  -t, --tag           build tagged release, for example: v3000 for specific release version,"
    echo "                      if PyPI doesn't contain tag, then utilizes git tag"
    echo "  -u, --user          username for git's salt and salt-pack,  and results NFS server directory, default saltstack"
    echo "                      Note: user's salt-pack changes should be against root for branch, for example: master"
    echo "  -v, --verbose       verbose output"
    echo "  -w, --mount_nfsdir  mount root NFS directory for minions as repository for build products"
    echo "  -y, --nfs_host      using user's NFS server hostname for repository for build products"
    echo "  -z, --nfs_absdir    absolute NFS directory on NFS server for mounting root NFS directory, for example: /volume3"
    echo ""
    echo "  creates specified packages for major platforms, signed with Salt testing keys"
    echo "  dated by current start time of execution in YYYYMMDDhhmmnnnn format"
    echo "  current platforms:"
    echo "      Redhat 8 and 7"
    echo "      Debian 11, 10, 9 and 8"
    echo "      Raspbian 9 and 8"
    echo "      Ubuntu 20.04, 18.04 and 16.04"
    echo ""
    echo "  script expects node to contain salt-master with auto_setup installed and"
    echo "  salt-minion installed on salt-master node, id 'm7m'"
    echo ""
    echo " Examples:"
    echo "          For Python 2 build of point release 3005, defaults to using master branch"
    echo "              autobuild -t v3005 --python2"
    echo ""
    echo "          For Python 3 build from a branch named mydevbranch for user joe from branch head"
    echo "              autobuild -b mydevbranch -u joe "
    echo ""
}


VERBOSE=0
CLEAN_BLD=0
DEBUG=false
USAGE_HELP=false
LOG_MODE='debug'
ATTEMPT_PYPI=0
RELEASE_TAG=''
RELEASE_BRANCH='master'
RELEASE_CODE_NAMED_BRANCH=''
RELEASE_SPECIFIC_NAME_BRANCH=''
RELEASE_DSIG_BRANCH=''
RELEASE_SPECIFIC_NAME_USER=''
RELEASE_SPECIFIC_PACK_BRANCH=''
RELEASE_SPECIFIC_USER_SALT_ONLY=false
PYTHON3_FLAG=true

## NFS Mount Defaults, note req. match values in auto_base_map.jinja
## DEFAULT_NFS_OPTS='bldressrv'
## ## DEFAULT_NFS_HOST='nas01.c7.saltstack.net'
## DEFAULT_NFS_HOST='10.1.0.98'
## DEFAULT_NFS_ABSDIR='/volume3'
## DEFAULT_MOUNT_NFSDIR='/mnt_build_res'

DEFAULT_NFS_OPTS='-t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,noresvport'
## dev DEFAULT_NFS_HOST='fs-6e0b6fc6.efs.us-west-2.amazonaws.com'
DEFAULT_NFS_HOST='fs-0a5660a2.efs.us-west-2.amazonaws.com'
DEFAULT_NFS_ABSDIR='/'
DEFAULT_MOUNT_NFSDIR='/mnt_build_res'

NFS_OPTS="${DEFAULT_NFS_OPTS}"
NFS_HOST="${DEFAULT_NFS_HOST}"
NFS_ABSDIR="${DEFAULT_NFS_ABSDIR}"
MOUNT_NFSDIR="${DEFAULT_MOUNT_NFSDIR}"

CLOUD_MAP=''
CLOUD_HOLD_FLAG=false

## minion resident on Salt-Master node
CODE_MINION='m7m'

### TBD THIS AREA IS TO BE REPLACED WITH VALUES FROM VAULT
### AND IS ONLY HERE AS A TEMPORARY STOP-GAP

SUBNETID='subnet-to-be-determined'
SECGROUPID='sec-group-to-be-determined'
AWS_ACCESS_PRIV_KEY_NAME='aws-file-key-name-to-be-determined'



##    -d | --debug )  DEBUG=True; shift ;;
##    -l | --log )  LOG_MODE="$2"; shift 2 ;;

## not validating input branch tag, format v2019.2.0 or 2019.2
BRANCH_TAG=''

while true; do
  case "${1}" in
    -b | --branch ) RELEASE_BRANCH="$2"; shift 2 ;;
    -c | --clean ) CLEAN_BLD=1; shift ;;
    -g | --cloud_map ) CLOUD_MAP="$2"; shift 2 ;;
    -h | --help ) USAGE_HELP=true; shift ;;
    -j | --cloud_hold ) CLOUD_HOLD_FLAG=true; shift ;;
    -m | --minion ) CODE_MINION="$2"; shift 2 ;;
    -n | --named_branch ) RELEASE_CODE_NAMED_BRANCH="$2"; shift 2 ;;
    -o | --user_salt_only) RELEASE_SPECIFIC_USER_SALT_ONLY=true; shift ;;
    -p | --pack_branch) RELEASE_SPECIFIC_PACK_BRANCH="$2"; shift 2 ;;
    -q | --python2 ) PYTHON3_FLAG=false; shift ;;
    -r | --nfs_opts) NFS_OPTS="$2"; shift 2 ;;
    -s | --specific_name) RELEASE_SPECIFIC_NAME_BRANCH="$2"; shift 2 ;;
    -t | --tag ) RELEASE_TAG="$2"; ATTEMPT_PYPI=1; shift 2 ;;
    -u | --username) RELEASE_SPECIFIC_NAME_USER="$2"; shift 2 ;;
    -v | --verbose ) VERBOSE=1; shift ;;
    -w | --mount_nfsdir ) MOUNT_NFSDIR="$2"; shift 2 ;;
    -y | --nfs_host ) NFS_HOST="$2"; shift 2 ;;
    -z | --nfs_absdir ) NFS_ABSDIR="$2"; shift 2 ;;
    -- ) shift; break ;;
    * ) break ;;
  esac
done

## check if want help, display usage and exit
[[ ${USAGE_HELP} = 'false' ]] || {
  _usage
  exit 0
}


## input version tag overrides use of branch if present
[[ -z "${RELEASE_TAG}" ]] && BRANCH_TAG="${RELEASE_BRANCH}" || BRANCH_TAG="${RELEASE_TAG}"

##  MAIN BODY OF SCRIPT

## build designation tag used for auto builds is YearMontDayHourMinuteSecondMicrosecond aka jid
date_long=$(date +%Y%m%d%H%M%S%N)
curr_date="${date_long::-2}"
APT_DATE="$(date +"%a, %d %b %Y %T %z")"
RPM_DATE="$(date +"%a %b %d %Y")"

# unique value for this run
UNIQUE_VALUE=$$

CURRDIR=$(pwd)

PILLAR_DIR='/srv/pillar'
PILLAR_AUTO_SETUP_DIR="auto_setup"
PILLAR_AUTO_SETUP_TAG_ABSFILE="${PILLAR_DIR}/${PILLAR_AUTO_SETUP_DIR}/tag_build_dsig.jinja"

if [[ ! -d "${PILLAR_DIR}/${PILLAR_AUTO_SETUP_DIR}" ]]; then
    echo "Missing vital directory ${PILLAR_DIR}/${PILLAR_AUTO_SETUP_DIR}, ensure system is correctly setup before proceeding"
    exit 1
fi

## ensure building designation is set for pillar data
if [[ -z ${RELEASE_SPECIFIC_NAME_BRANCH} ]]; then
    if [[ -n ${RELEASE_TAG} ]]; then
        RELEASE_DSIG_BRANCH=$(echo "${RELEASE_TAG}" | sed 's/v//')
    else
        RELEASE_DSIG_BRANCH="nb${curr_date}"
    fi
else
    RELEASE_DSIG_BRANCH=${RELEASE_SPECIFIC_NAME_BRANCH}
fi

## output Branch signifier
echo "Release Branch Tag is \'${BRANCH_TAG}\'"
echo "Release DSIG Branch is \'${RELEASE_DSIG_BRANCH}\'"


cat <<@EOF > "${PILLAR_AUTO_SETUP_TAG_ABSFILE}"
{% set branch_tag = '${BRANCH_TAG}' %}
{% set build_dsig = '${RELEASE_DSIG_BRANCH}' %}
{% set build_local_minion = '${CODE_MINION}' %}
{% set nfs_opts = '${NFS_OPTS}' %}
{% set nfs_host = '${NFS_HOST}' %}
{% set nfs_absdir = '${NFS_ABSDIR}' %}
{% set mount_nfsdir = '${MOUNT_NFSDIR}' %}
{% set build_apt_date = '${APT_DATE}' %}
{% set build_rpm_date = '${RPM_DATE}' %}
{% set build_py3 = ${PYTHON3_FLAG} %}
{% set build_cloud_hold = ${CLOUD_HOLD_FLAG} %}
{% set uniqueval = ${UNIQUE_VALUE} %}
{% set subnet_id = '${SUBNETID}' %}
{% set sec_group_id = '${SECGROUPID}' %}
{% set aws_access_priv_key_name = '${AWS_ACCESS_PRIV_KEY_NAME}' %}
{% set specific_name_user_salt_only = ${RELEASE_SPECIFIC_USER_SALT_ONLY} %}
@EOF

if [[ ${CLEAN_BLD} -ge 1 ]]; then
cat <<@EOF >> "${PILLAR_AUTO_SETUP_TAG_ABSFILE}"
{% set build_clean = '${CLEAN_BLD}' %}
@EOF
fi

if [[ -n "${RELEASE_CODE_NAMED_BRANCH}" ]]; then
cat <<@EOF >> "${PILLAR_AUTO_SETUP_TAG_ABSFILE}"
{% set code_named_branch_tag = '${RELEASE_CODE_NAMED_BRANCH}' %}
@EOF
fi

if [[ -n "${RELEASE_SPECIFIC_NAME_BRANCH}" ]]; then
cat <<@EOF >> "${PILLAR_AUTO_SETUP_TAG_ABSFILE}"
{% set specific_name_branch_tag = '${RELEASE_SPECIFIC_NAME_BRANCH}' %}
@EOF
fi

if [[ -n "${RELEASE_SPECIFIC_NAME_USER}" ]]; then
cat <<@EOF >> "${PILLAR_AUTO_SETUP_TAG_ABSFILE}"
{% set specific_name_user = '${RELEASE_SPECIFIC_NAME_USER}' %}
@EOF
fi

if [[ -n "${RELEASE_SPECIFIC_PACK_BRANCH}" ]]; then
cat <<@EOF >> "${PILLAR_AUTO_SETUP_TAG_ABSFILE}"
{% set specific_pack_branch = '${RELEASE_SPECIFIC_PACK_BRANCH}' %}
@EOF
fi

if [[ -n "${CLOUD_MAP}" ]]; then
cat <<@EOF >> "${PILLAR_AUTO_SETUP_TAG_ABSFILE}"
{% set build_cloud_map = '${CLOUD_MAP}' %}
@EOF
fi


# set logging infomation
## want verbose while developing
LOGGING="/dev/null"
SCRIPTNAME=$(basename "$0")
log_file="/var/log/salt/$SCRIPTNAME-${curr_date}.log"

if [[ ${VERBOSE} -ne 0 ]];then
    LOGGING="${log_file}"
else
    LOGGING="/dev/null"
fi


_display "$SCRIPTNAME: autobuild started"

salt ${CODE_MINION} saltutil.refresh_pillar || {
    _error "$SCRIPTNAME: refresh pillar for '${BRANCH_TAG}', retcode '${$?}'";
}

# setup environment and reactors
salt ${CODE_MINION} state.sls auto_setup.setup_envir || {
    _error "$SCRIPTNAME:state for auto_setup setup_envir failed to function for '${BRANCH_TAG}', retcode '${$?}'";
}

# ensure reactor changes are read and give time for salt-minion to come back up
systemctl restart salt-master
sleep 10
systemctl restart salt-minion
sleep 10

salt ${CODE_MINION} state.sls auto_setup.setup_local_mount || {
    _warning "$SCRIPTNAME:state for auto_setup setup_local_mount failed to function for '${BRANCH_TAG}', retcode '${$?}'";
}

salt ${CODE_MINION} state.sls auto_setup.setup_tagged_from_pypi || {
    _warning "$SCRIPTNAME:state for auto_setup setup tagged version from PyPI failed for '${BRANCH_TAG}', retcode '${$?}'";
}

salt ${CODE_MINION} state.sls auto_setup || {
    _error "$SCRIPTNAME:state for auto_setup failed to function for '${BRANCH_TAG}', retcode '${$?}'";
}

salt ${CODE_MINION} state.sls auto_setup.setup_vault_user || {
    _error "$SCRIPTNAME:state for auto_setup user vault keys for '${BRANCH_TAG}', retcode '${$?}'";
}

# refresh again after any updates from vault values to pillar
salt ${CODE_MINION} saltutil.refresh_pillar || {
    _error "$SCRIPTNAME: secondary refresh pillar for '${BRANCH_TAG}', retcode '${$?}'";
}

salt ${CODE_MINION} state.sls auto_setup.setup_keyid || {
    _error "$SCRIPTNAME:state for auto_setup keyid from vault for '${BRANCH_TAG}', retcode '${$?}'";
}

_display "$SCRIPTNAME: autobuild checked out and modified salt from '${BRANCH_TAG}' and about to build via orchstration"

_display "$SCRIPTNAME: autobuild ensure build minions are up and running"

salt ${CODE_MINION} state.sls auto_setup.setup_cloud || {
    _error "$SCRIPTNAME:state for auto_setup failed to setup cloud for '${BRANCH_TAG}', retcode '${$?}'";
}

_display "$SCRIPTNAME: autobuild via orchstration started for modified salt from '${BRANCH_TAG}' on build minions from cloud map ${CLOUD_MAP}"

# get list from salt-cloud and remove [ and ] on first and last line
## node_list=$( salt-cloud -l quiet -y -P -m /etc/salt/cloud.map -Q  --out=json | jq '.[] | .[] | keys' | tail -n +2 | head -n -1)   ## for opennebula

## running into issues with Ubuntu 18.04/20.04 and being called async when starting async, going to try leaving it to last and call directly
## node_list=$( salt-cloud -l quiet -y -P -m /etc/salt/cloud.map -Q  --out=json | jq  '.[] | .[] | .[] | .name' | sort -r )       ## for ec2
node_list=$(salt-cloud -l quiet -y -P -m /etc/salt/cloud.map -Q  --out=json | jq  '.[] | .[] | .[] | .name' | sort)       ## for ec2
rm ~/mybuild-"${BRANCH_TAG}"-svc-builder-autotest-*.log || true

for ndx in ${node_list}
do
    nx=$(echo "${ndx}" | sed  's/,//' | sed 's/"//g')
    salt "${nx}" saltutil.refresh_grains

    ## work-around for some needed packages installed which are unavailable from the OS or Salt currently (first time - bootstrap)
    if [[ -d bootstrap-pkgs ]]; then
        if [[ $nx == svc-builder-autotest-c8m* ]]; then
            if [[ ( ${PYTHON3_FLAG} ) && (-f './bootstrap-pkgs/python3-gnupg-0.4.4-2.el8.noarch.rpm') ]]; then
                salt $nx pkg.install  reinstall=True sources='[{"python3-gnupg": "salt://auto_setup/bootstrap-pkgs/python3-gnupg-0.4.4-2.el8.noarch.rpm"}]'
            fi
        elif [[ $nx == svc-builder-autotest-c7m* ]]; then
            if [[ ( ${PYTHON3_FLAG} ) && (-f './bootstrap-pkgs/python36-gnupg-0.4.4-2.el7.noarch.rpm') ]]; then
                salt $nx pkg.install reinstall=True sources='[{"python36-gnupg": "salt://auto_setup/bootstrap-pkgs/python36-gnupg-0.4.4-2.el7.noarch.rpm"}]'
            fi
        fi
    fi

    if [[ $nx == svc-builder-autotest-u1804m* ]]; then
        salt ${nx} state.sls auto_setup.ubuntu_wrkaround
    fi
    if [[ $nx == svc-builder-autotest-d9m* ]]; then
        salt ${nx} state.sls auto_setup.debian_wrkaround
    fi
    salt-run state.orchestrate auto_setup.orch.build_platform_common pillar="{'minion_tgt':"${nx}"}" 2>&1 > ~/mybuild-"${BRANCH_TAG}-${nx%-*}".log &

done

_display "$SCRIPTNAME: autobuild finished"

